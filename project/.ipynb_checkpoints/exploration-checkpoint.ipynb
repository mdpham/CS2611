{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is necessary to include the project directory into system paths\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can import from project directory\n",
    "from project.src.data_classes import (\n",
    "    SCOTUS,\n",
    "    Opinion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for analysis, we want to abstract functionality into classes that can be imported above as we test/develop in Jupyter \n",
    "# This way, we work with classes (for Juypter use) and build out from there\n",
    "# Try and build classes that encapsulates different constitute parts of the NLP\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import combinations\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usecols = ['author_name', 'category', 'per_curiam', 'case_name', 'year_filed', 'text']\n",
    "# since_1970 = pd.read_csv('scotus/opinions_since_1970.csv', usecols=usecols)\n",
    "# all_opinions = pd.read_csv('scotus/all_opinions.csv', usecols=usecols)\n",
    "scotus_data = SCOTUS()\n",
    "all_opinions = scotus_data.all_opinions.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/emaadmanzoor/1d06e0751a3f7d39bc6814941b37531d\n",
    "test_text = all_opinions.head().at[0, 'text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counters for each justice encounter\n",
    "\n",
    "class JusticeOpinionCounter:\n",
    "\n",
    "    def __init__(self, author_name):\n",
    "        self.texts = []\n",
    "        self.unigram_counter = Counter()\n",
    "        self.bigram_counter = Counter()\n",
    "        self.trigram_counter = Counter()\n",
    "\n",
    "    def add_opinion(self, text):\n",
    "        self.texts.append(text)\n",
    "     \n",
    "        #         Clean and tokenize\n",
    "        #         obvious-ly => obvious ly (lemmatization)\n",
    "        words = ''.join((filter(lambda x: x in string.printable, text))).replace('\\n', ' ')\n",
    "        # print(type(words), len(words))\n",
    "        words = words.replace('–', '')\n",
    "        # print(type(words), len(words))\n",
    "        table = str.maketrans('', '', string.punctuation+'’‘'+'“”'+'–'+string.digits+'­')\n",
    "        words = [w.translate(table).lower() for w in words.split()]\n",
    "        # print(type(words), len(words))\n",
    "        stop_words = stopwords.words('english')\n",
    "        words = list(filter(lambda w: w not in stop_words, words))\n",
    "        # print(type(words), len(words))\n",
    "        words = list(filter(lambda w: w.isalpha(), words))\n",
    "        \n",
    "        self.unigram_counter.update(list(ngrams(words, 1)))\n",
    "        self.bigram_counter.update(list(ngrams(words, 2)))\n",
    "        self.trigram_counter.update(list(ngrams(words, 3)))\n",
    "        # print(len(words), len(self.unigram_counter.keys()), len(self.bigram_counter.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for x, op in all_opinions[['author_name', 'text']][0:10].iterrows():\n",
    "    print()\n",
    "    if (op['author_name'] == 'Justice Thomas'):\n",
    "        print(op['text'])\n",
    "        # print(len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def opinion_counter(df):\n",
    "    op_counters = {}\n",
    "    for i, op in df[['author_name', 'text']][:1000].iterrows():\n",
    "        author_name = op['author_name']\n",
    "        if author_name not in op_counters:\n",
    "            op_counters[author_name] = JusticeOpinionCounter(author_name)\n",
    "        op_counters[author_name].add_opinion(op['text'])\n",
    "    return op_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "op_counters = opinion_counter(all_opinions)\n",
    "len(all_opinions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_barchart(most_common, author_name):\n",
    "    fig, ax = plt.subplots()\n",
    "    # Example data\n",
    "    tokens = list(map(lambda token_freq: token_freq[0], most_common))\n",
    "    y_pos = np.arange(len(tokens))\n",
    "    freqs = list(map(lambda token_freq: token_freq[1], most_common))\n",
    "    \n",
    "    ax.barh(y_pos, freqs, align='center')\n",
    "    ax.set_yticks(y_pos, labels=tokens)\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_xlabel('Frequency')\n",
    "    ax.set_title(f'{author_name} ngram frequency')\n",
    "    plt.show()\n",
    "    \n",
    "def most_common_barchart(op_counters, author_name, top=20):\n",
    "    most_common_uni = op_counters[author_name].unigram_counter.most_common()[:top]\n",
    "    most_common_bi = op_counters[author_name].bigram_counter.most_common()[:top]\n",
    "    most_common_tri = op_counters[author_name].trigram_counter.most_common()[:top]\n",
    "    ngram_barchart(most_common_uni, author_name)\n",
    "    ngram_barchart(most_common_bi, author_name)\n",
    "    ngram_barchart(most_common_tri, author_name)\n",
    "    \n",
    "    \n",
    "most_common_barchart(op_counters, 'Justice Roberts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_common(op_counters, author_name, top=10):\n",
    "    min_token_len = lambda w: len(w) >= 1\n",
    "    counter = op_counters[author_name]\n",
    "    print(author_name)\n",
    "    print('\\t unigrams:')\n",
    "    [print(f'\\t\\t{k} {v}') for k,v in counter.unigram_counter.most_common(top)]\n",
    "    print('\\t bigrams:')\n",
    "    [print(f'\\t\\t{k} {v}') for k,v in counter.bigram_counter.most_common(top) if all(map(min_token_len, k))]\n",
    "    print('\\t trigrams:')\n",
    "    [print(f'\\t\\t{k} {v}') for k,v in counter.trigram_counter.most_common(top) if all(map(min_token_len, k))]\n",
    "    \n",
    "print_most_common(op_counters, 'Justice Roberts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for x, op in all_opinions[['author_name', 'text']][0:10].iterrows():\n",
    "#     print()\n",
    "#     if (op['author_name'] == 'Justice Thomas'):\n",
    "#         print(op['text'])\n",
    "#         # print(len())\n",
    "\n",
    "def unigram_wordcloud(op_counters, author_name):\n",
    "    wordcloud_unigrams = dict([[k[0],v] for k,v in op_counters[author_name].unigram_counter.items()])\n",
    "    wordcloud = WordCloud(background_color=None, mode='RGBA', max_words=100).generate_from_frequencies(wordcloud_unigrams)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "unigram_wordcloud(op_counters, 'Justice Roberts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def most_common_unigram_heatmap(op_counters, author_name, top=10):\n",
    "    min_token_len = 3\n",
    "    most_common = op_counters[author_name].unigram_counter.most_common()\n",
    "    top_unigrams = [k[0] for k, v in most_common if len(k[0]) >= min_token_len][:top]\n",
    "    i2t = dict([[k,v] for k,v in enumerate(top_unigrams)])\n",
    "    heatmap_bigram = np.zeros((top,top), int)\n",
    "    for i in range(top):\n",
    "        for j in range(top):\n",
    "            heatmap_bigram[i,j] = op_counters[author_name].bigram_counter[(i2t[i], i2t[j])]\n",
    "    heatmap_bigram = np.log(heatmap_bigram + 1)\n",
    "    ax = sns.heatmap(heatmap_bigram, linewidth=0.5,  xticklabels=top_unigrams, yticklabels=top_unigrams, cbar_kws={'label': 'log(freq + 1)'})\n",
    "    plt.title(f'Heatmap of bigrams for top {top} unigrams (min {min_token_len} chars)')\n",
    "    plt.show()\n",
    "    return heatmap_bigram\n",
    "            \n",
    "def most_common_bigram_heatmap(op_counters, author_name, top=10):\n",
    "    min_token_len = 3\n",
    "    most_common = op_counters[author_name].bigram_counter.most_common()\n",
    "    tokens = []\n",
    "    for bigram, freq in most_common:\n",
    "        for token in bigram:\n",
    "            if token not in tokens and len(token) >= min_token_len:\n",
    "                tokens.append(token)\n",
    "    top_tokens = tokens[:top]\n",
    "    \n",
    "    i2t = dict([[k,v] for k,v in enumerate(top_tokens)])\n",
    "    heatmap_bigram = np.zeros((top,top), int)\n",
    "    for i in range(top):\n",
    "        for j in range(top):\n",
    "            heatmap_bigram[i,j] = op_counters[author_name].bigram_counter[(i2t[i], i2t[j])]\n",
    "    # heatmap_bigram = np.log(heatmap_bigram + 1)\n",
    "    ax = sns.heatmap(heatmap_bigram, linewidth=0.5, xticklabels=top_tokens, yticklabels=top_tokens, cbar_kws={'label': 'freq'})\n",
    "    plt.title(f'Heatmap of top {top} bigrams (min {min_token_len} chars)')\n",
    "    plt.show()\n",
    "    return heatmap_bigram\n",
    "\n",
    "most_common_unigram_heatmap(op_counters, 'Justice Roberts')\n",
    "most_common_bigram_heatmap(op_counters, 'Justice Roberts')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(op_counters, author_name):\n",
    "    print_most_common(op_counters, author_name)\n",
    "    most_common_barchart(op_counters, author_name)\n",
    "    unigram_wordcloud(op_counters, author_name)\n",
    "    most_common_unigram_heatmap(op_counters, author_name)\n",
    "    most_common_bigram_heatmap(op_counters, author_name)\n",
    "summarize(op_counters, 'Justice Pitney')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_justices(df):\n",
    "    justices = list(set(df['author_name']))\n",
    "    print(f'{len(justices)} opinion authors')\n",
    "    # [print(f'\\t{j}') for j in justices]6\n",
    "    return justices\n",
    "\n",
    "def print_cases(df):\n",
    "    cases = list(set(df['case_name']))\n",
    "    print(f'{len(cases)} cases')\n",
    "    # [print(f'\\t{c}') for c in cases]\n",
    "    return cases\n",
    "\n",
    "def select_opinions_df(df, author_names=None, case_names=None):\n",
    "    # Filter opinions that are not 'per_curiam' (by the court)\n",
    "    opinions = df[df['per_curiam'] == False]\n",
    "    if author_names is not None:\n",
    "        opinions = opinions.loc[opinions['author_name'].isin(author_names)]\n",
    "    if case_names is not None:\n",
    "        opinions = opinions.loc[opinions['case_name'].isin(case_names)]\n",
    "    \n",
    "    cases = print_cases(opinions)\n",
    "    justices = print_justices(opinions)\n",
    "    return cases, justices, opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/List_of_landmark_court_decisions_in_the_United_States#Birth_control_and_abortion\n",
    "landmark_cases = [\n",
    "    \"Griswold v. Connecticut\",\n",
    "    \"Eisenstadt v. Baird\",\n",
    "    \"Roe v. Wade\",\n",
    "    \"Carey v. Population Services International\",\n",
    "    \"Planned Parenthood v. Casey\",\n",
    "    \"Stenberg v. Carhart\",\n",
    "    \"Gonzales v. Carhart\",\n",
    "    \"Burwell v. Hobby Lobby Stores, Inc.\",\n",
    "    \"Whole Woman's Health v. Hellerstedt\"\n",
    "]\n",
    "author_names = None\n",
    "# cases = print_cases(all_opinions)\n",
    "# for case_name in landmark_cases:\n",
    "    # print(f'{case_name} available: {case_name in cases}') \n",
    "cases, justices, opinions = select_opinions_df(all_opinions, author_names=author_names, case_names=landmark_cases)\n",
    "landmark_counter = opinion_counter(opinions)\n",
    "for j in justices:\n",
    "    summarize(landmark_counter, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cases(all_opinions.loc[all_opinions['year_filed'].isin([1977, 1976, 1978])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Justices appointed by party: (Democratic, Republican)\n",
    "Category of opinion: (Dissenting , Concurring, Majority)\n",
    "\n",
    "For each case:\n",
    "    determine which 'party' had majority justices for a decision\n",
    "    According to asym poli: there should be less agreement across parties as time goes on\n",
    "        Polarization => Asymmetric polarization (levels of analysis)\n",
    "            If it polarizes, is it asymmetric?\n",
    "        For different presidential terms (political mood): split cases by presidential term and evaluate stats\n",
    "    Do certain decisions/categories occur for certain presidential party?\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two dimensions of analysis: across time (for landmark cases) and within decisions (across categories)\n",
    "\n",
    "For within decisions: how do words cluster?\n",
    "\n",
    "For across time: alignment of word embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
